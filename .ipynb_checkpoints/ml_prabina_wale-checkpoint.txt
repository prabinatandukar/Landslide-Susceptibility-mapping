import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import balanced_accuracy_score, roc_auc_score,make_scorer
from sklearn.model_selection import GridSearchCV
#from sklearn.preprocessing import OneHotEncoder, LabelEncoder

from yellowbrick.classifier import ClassificationReport
from yellowbrick.classifier import ConfusionMatrix



X=data.drop('label', axis=1).copy()
y=data.loc[:,'label'].copy()

X.dtypes
cols=X.columns[X.dtypes=='object'].values.tolist()
cols
X_encoded=pd.get_dummies(X,columns=cols)
X_train, X_test, y_train, y_test=train_test_split(X_encoded,y, random_state=123456, train_size=0.70, stratify=y)

clf_xgb=xgb.XGBClassifier(objective='binary:logistic', missing=np.nan,eval_metric='aucpr', early_stopping_rounds=10,seed=42)

clf_xgb.fit(X_train,y_train,verbose=True,eval_set=[(X_test,y_test)])



sns.set(style="white",font_scale=1.4, rc={"font.weight": "bold"})
sns.set_style(style="white")
#cm = ConfusionMatrix(clf_xgb, classes=list(class_mapper.keys()))
fig,ax=plt.subplots(figsize=(15,5))
cm = ConfusionMatrix(clf_xgb, classes=list(class_mapper.keys()),cmap="Greys")
cm.fit(X_train,y_train)
cm.score(X_test,y_test)
cm.show()



param_grid={'max_depth':[3,4,5],
    'learning_rate':[0.1,0.01,0.05],
    'gamma':[0,0.25,1.0],
    'reg_lambda':[0,1.0,10.0],
    'scale_pos_weight':[1,3,5]}

optimal_params=GridSearchCV(estimator=xgb.XGBClassifier(objective='binary:logistic', seed=42,subsample=0.9,colsample=0.5),
                           param_grid=param_grid,
                           scoring='roc_auc',verbose=2,n_jobs=8,cv=3)



optimal_params.fit(X_train,y_train,
                   early_stopping_rounds=10,
                   eval_metric='auc',
                   verbose=True,
                   eval_set=[(X_test,y_test)])


optimal_params.best_params_



clf_xgb=xgb.XGBClassifier(objective='binary:logistic', missing=0,eval_metric='aucpr', early_stopping_rounds=10,seed=42,
                          **optimal_params.best_params_)


clf_xgb.fit(X_train,y_train,verbose=True,eval_set=[(X_test,y_test)])



cm = ConfusionMatrix(clf_xgb, classes=list(class_mapper.keys()))
#cm = ConfusionMatrix(clf_xgb, classes=list(class_mapper.keys()),cmap="Greys")
cm.fit(X_train,y_train)
cm.score(X_test,y_test)
cm.show()



from yellowbrick.model_selection import FeatureImportances

fig,ax=plt.subplots(figsize=(25,15))
viz = FeatureImportances(clf_xgb, relative=True, ax=ax)

viz.fit(X_train, y_train)
ax.set(title="Relative features importance", ylabel="Features", xlabel="Relative Importance")
fig.tight_layout()



original_palette = sns.color_palette("Greys")

# Create a reversed version of the palette
reversed_palette = original_palette[::-1]


#might get erro here 
visualizer = ClassificationReport(clf_xgb, classes=list(class_mapper_full.keys()), support=False,cmap=reversed_palette)


visualizer = ClassificationReport(clf_xgb, classes=list(class_mapper_full.keys()), support=False,cmap=reversed_palette)
sns.set(style="white",font_scale=2.5, rc={"font.weight": "bold"})
sns.set_style(style="white")
#cm = ConfusionMatrix(clf_xgb, classes=list(class_mapper.keys()))
fig,ax=plt.subplots(figsize=(25,15))

visualizer.fit(X_train, y_train)        # Fit the visualizer and the model
visualizer.score(X_test, y_test)        # Evaluate the model on the test data
visualizer.show(outpath="./thesis_results/accuracy_assessment.png")    
ann=["b"]    
for a,_ann in zip([ax], ann):    
    x,y=annotate_position(ax=a,xy_loc=(-0.1,1))
    #a.text(x,y,f'({_ann})',ha='center', va='center')
cm.show()
#fig.tight_layout()